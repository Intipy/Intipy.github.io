---
title:  "교차 엔트로피 오차(Cross Entropy Error, CEE)에 대하여"

categories:
  - AI
last_modified_at: 2021-10-12T08:06:00-05:00

---


교차 엔트로피를 알아보기 전에 엔트로피에 대해 알아보겠다.

엔트로피란 불확실성을 나타내는 지표라고 할 수 있다.

불확실하다는 것은 무엇인가?

불확실이란 '예측하기 어려움' 이라고 할 수 있다.

예를 들어,

어떤 시험 문제의 1번부터 5번까지의 5문항 중에서 하나가 정답이라고 하자.

이 경우에 불확실성은 높다.

어떤 문제가 정답인지 예측하기 어렵기 때문이다.

하지만 문항이 2개라면 맞출 확률은 올라간다.

문제를 더 예측할 수 있게 된 것이다.

이때 불확실성은 줄어든다.

만약 문항이 1개라면?

이때는 불확실성이 없다.

확실하게 정답을 예측할 수 있기 때문이다.

이렇게 불확실성이 높으면 엔트로피가 높다고 한다.

이것을 확률질량함수와 확률변수를 통해 표현하면 아래와 같다.

$H(p) = -\sum^{n}_{i=1} p(x_i) \log_b p(x_i)$

p는 확률질량함수이며 $x_i$일 확률과 그 로그를 취한 것을 곱하여 더한다.

예시를 들어보자.

어떤 상자에 검은공과 하얀공이 9:1 비율로 섞여있으며, 이것을 꺼낸다고 하자.

검은공이 나올 확률은 0.9, 하얀공이 나올 확률은 0.1이다.

이때 엔트로피를 구하면 아래와 같다.

$-(0.9 \log_e 0.9 + 0.1 \, \log_e 0.1)=0.325...$

그렇다면 1:1 비율로 섞여있다면 어떨까?

각각 뽑힐 확률이 0.5이다.

엔트로피는 아래와 같이 구해진다.

$- (0.5 \log_e 0.5 + 0.5 \log_e 0.5) =0.693...$

후자의 엔트로피가 전자보다 높게 나오는 것을 볼 수 있다.

앞서 말한 것처럼 엔트로피는 불확실성을 나타내며,

이는 후자의 불확실성이 높다는 뜻이다.

즉, 후자는 예측하기 어렵다.

전자처럼 9:1 비율이라면 검은공이 훨씬 많으므로 검은공이 뽑힐 확률이 높다.

즉, 우리는 검은공이 뽑힐 것이라고 '예측'을 할 수 있다.

예측을 더 잘할 수 있다는 것은 불확실성이 낮다는 뜻이다.

후자처럼 확률이 50%로 나눠지는 경우에는 어느 하나가 뽑힐 것이라고 예측하기 어렵다.

예측이 더 어려우므로 불확실성이 높다.

따라서 엔트로피가 더 높은 것이다.

이제 엔트로피에 대해 정리해보겠다.

1. 엔트로피란 불확실성의 척도이다.
2. 불확실성이란 예측하기 어려운 정도라고도 할 수 있다.
3. 예측이 어려울수록 불확실하며, 엔트로피가 높다.
4. 확률이 한곳에 몰려있으면 예측하기 쉬우며 엔트로피가 낮다.
5. 확률이 여러곳에 분산되어 있으면 예측하기 어려우며 엔트로피가 높다.

엔트로피, 불확실성, 예측하기 어려운 정도, 확률이 퍼져있는 정도

이 4가지가 하나를 표현하고 있음을 아는 것이 핵심이다.



이제 교차 엔트로피에 대해 알아보겠다.

앞에서 본 엔트로피는 하나의 확률분포를 통해 정의되었다.

교차 엔트로피도 이와 거의 비슷한데,

교차 엔트로피의 경우에는 2개의 확률분포에 대하여 엔트로피를 정의한다.

교차 엔트로피는 두 분포가 얼마나 비슷한지를 나타낸다고 볼 수 있다.

분포 p와 또다른 분포 q를 설정했을 때, 

교차 엔트로피를 다음과 같이 정의할 수 있다.

$H_q(p) = -\sum^{n}_{i=1} p(x_i) \log_b q(x_i)$


p를 실제 분포(정답 분포)라 하고, q를 모사 분포(예측 분포)라고 할 수도 있다.


분포가 0.2 / 0.2 / 0.6 일 때,

0.3 / 0.3 / 0.4 로 분포를 예측하였다고 하자.

이때의 엔트로피와 교차 엔트로피는 아래와 같다.

$H(p) = -(0.2\log_e0.2 + 0.2\log0.2 + 0.6\log0.6) = 0.950...$

$H_q(p) = -(0.2\log_e0.3 + 0.2\log_e0.3 + 0.6\log_e0.4)=1.031...$



교차 엔트로피에 대해 더 알아보기 위해 쿨백-라이블러 발산(Kullback-Leibler Divergence)이라는 것을 알아보자.

KL 발산은 두 분포의 차이를 측정하는 것인데, 교차 엔트로피에서 엔트로피를 빼준 것이다.

$$D_{KL}(p||q) = -\sum^{n}_{i=1}p(x_i) \Biggl(\log_e q(x_i) - \log_e p(x_i) \Biggr)= H_q(p) - H(p)$$


$H_q(p) \geq H(p)$ 이므로 $H_q(p) - H(p) \geq 0$

$$D_{KL}(p||q)$$의 값은 최소 0이며, 예측 분포가 정답에 가까워질수록 0으로 수렴한다.




