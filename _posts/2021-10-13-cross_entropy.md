---
title:  "교차 엔트로피 오차(Cross Entropy Error, CEE)에 대하여"

categories:
  - Machine Learning 
last_modified_at: 2021-10-12T08:06:00-05:00

---

교차 엔트로피를 알아보기 전에 엔트로피에 대해 알아보겠다.
엔트로피란 불확실성을 나타내는 지표라고 할 수 있다.

불확실하다는 것은 무엇인가?
불확실이란 '예측하기 어려움' 이라고 할 수 있다.

예를 들어,
어떤 시험 문제의 1번부터 5번까지의 다섯 문항 
중에서 하나가 정답이라고 해보자.
이 경우에 불확실성은 높다.
어떤 문제가 정답인지 예측하기 어렵기 때문이다.

하지만 문항이 2개라면 맞출 확률은 올라간다.
문제를 더 예측할 수 있게 된 것이다.
이때 불확실성은 줄어든다.

만약 문항이 1개라면?
이때는 불확실성이 없다.
확실하게 정답을 예측할 수 있기 때문이다.
이렇게 불확실성이 높으면 엔트로피가 높다고 한다.

이것을 확률질량함수와 확률변수를 통해 표현하면 아래와 같다.

$$
\begin{align} 
\tag{1}
H(p) = -\sum^{n}_{i=1} p(x_i) \log_b p(x_i)
\end{align} 
$$

p는 확률질량함수이며 $x_i$일 확률과 그 로그를 취한 것을 곱하여 더한다.

예시를 들어보자.
어떤 상자에 검은공과 하얀공이 9:1 
비율로 섞여있으며, 이것을 꺼낸다고 하자.
검은공이 나올 확률은 0.9, 하얀공이 나올 확률은 0.1이다.
이때 엔트로피를 구하면 아래와 같다.

$$
\begin{align} 
\tag{2}
-(0.9 \log_e 0.9 + 0.1 \, \log_e 0.1)=0.325...
\end{align} 
$$

그렇다면 1:1 비율로 섞여있다면 어떨까?
각각 뽑힐 확률이 0.5이다.
엔트로피는 아래와 같이 구해진다.

$$
\begin{align} 
\tag{3}
-(0.5 \log_e 0.5 + 0.5 \log_e 0.5) =0.693...
\end{align} 
$$

후자(3)의 엔트로피가 전자(2)보다 높게 나오는 것을 볼 수 있다.
앞서 말한 것처럼 엔트로피는 불확실성을 나타내며,
이는 후자의 불확실성이 높다는 뜻이다.
즉, 후자는 예측하기 어렵다.

전자처럼 9:1 비율이라면 검은공이 훨씬 많으므로 
검은공이 뽑힐 확률이 높다.
즉, 우리는 검은공이 뽑힐 것이라고 
'예측'을 할 수 있다.
예측을 더 잘할 수 있다는 것은 
불확실성이 낮다는 뜻이다.

후자처럼 확률이 50%로 나눠지는 경우에는 
어느 하나가 뽑힐 것이라고 예측하기 어렵다.
예측이 더 어려우므로 불확실성이 높다.
따라서 엔트로피가 더 높은 것이다.

이제 엔트로피에 대해 정리해보겠다.

1. 엔트로피란 불확실성의 척도이다.
2. 불확실성이란 예측하기 어려운 정도라고도 할 수 있다.
3. 예측이 어려울수록 불확실하며, 엔트로피가 높다.
4. 확률이 한곳에 몰려있으면 예측하기 쉬우며 엔트로피가 낮다.
5. 확률이 여러곳에 분산되어 있으면 예측하기 어려우며 엔트로피가 높다.

엔트로피, 불확실성, 예측하기 어려운 정도, 확률이 퍼져있는 정도.
이 4가지가 하나를 표현하고 있음을 알도록 하자.

이제 교차 엔트로피에 대해 알아보겠다.
앞에서 본 엔트로피는 하나의 확률분포를 통해 정의되었다.
교차 엔트로피도 이와 거의 비슷한데,
교차 엔트로피의 경우에는 2개의 확률분포에 
대하여 엔트로피를 정의한다.
교차 엔트로피는 두 분포가 얼마나 비슷한지를 
나타낸다고 볼 수 있다.

분포 p와 또다른 분포 q를 설정했을 때, 
교차 엔트로피를 다음 (4)와 같이 정의할 수 있다.

$$
\begin{align} 
\tag{4}
H_q(p) = -\sum^{n}_{i=1} p(x_i) \log_b q(x_i)
\end{align}
$$

p를 실제 분포(정답 분포)라 하고, q를 모사 분포(예측 분포)라고 할 수도 있다.

분포가 0.2 / 0.2 / 0.6 일 때,
0.3 / 0.3 / 0.4 로 분포를 예측하였다고 하자.
이때의 엔트로피와 교차 엔트로피는 각각 (5), (6)과 같이 구해진다. 

$$
\begin{align} 
\tag{5}
H(p) = -(0.2\log_e0.2 + 0.2\log0.2 + 0.6\log0.6) = 0.950...
\end{align}
$$

$$
\begin{align} 
\tag{6}
H_q(p) = -(0.2\log_e0.3 + 0.2\log_e0.3 + 0.6\log_e0.4)=1.031...
\end{align}
$$

교차 엔트로피에 대해 더 알아보기 위해 쿨백-라이블러 발산(Kullback-Leibler Divergence)이라는 것을 알아보자.
KL 발산은 두 분포의 차이를 측정하는 것인데, 교차 엔트로피에서 엔트로피를 빼준 것이다.
즉, KL 발산이 작아지도록 하는 예측값이 정답에 가까워지는 예측값이다.

$$
\begin{align} 
\tag{7}
D_{KL}(p||q) = -\sum^{n}_{i=1}p(x_i) \Biggl(\log_e q(x_i) - \log_e p(x_i) \Biggr)= H_q(p) - H(p)
\end{align}
$$

<br/>

$$
\begin{align}
H_q(p) \geq H(p)
\end{align}
$$

$$
\begin{align}
\tag{8}
\therefore H_q(p) - H(p) \geq 0
\end{align}
$$

(8)에 따라 $D_{KL}$의 값은 최소 0이며, 예측 분포가 정답에 가까워질수록 0으로 수렴한다.
하지만 현실에서는 정답의 확률이 고루 분포된 형태로 나타나진 않는다.
사과 사진에 대해 사과라는 정답을 부여하는 것이지 "사과일 확률 80% 포도일 확률 20%" 같은 정답은 없다.
정답은 하나로 정해져야 한다는 뜻이다.

즉, 엔트로피가 0이며 쿨백-라이블러 발산이 $H_q(p) - 0$으로 계산된다.
따라서 $D_{KL} = H_q(p)$ 라는 등식이 성립하며 교차 엔트로피가 곧 $D_{KL}$가 된다.
<br/>
<br/>
마지막으로 엔트로피에서 로그를 취하는 이유에 대해 알아보겠다. 
엔트로피란 경우의 수로도 볼 수 있다.
이 경우의 수는 곱의 법칙에 의해 증가하는데, 예를 들어 동전의 경우
1개의 동전에서는 2개의 경우의 수가, 2개의 동전에서는 $2^2$의 경우의 수가 생긴다.
<br/>
<br/>
이렇게 경우의 수가 제곱으로(지수적으로) 증가하기 때문에 로그를 취해 선형적 관계를 유지할 수 있다.
또한 로그의 성질 중 하나인 $\log_axy = \log_ax + \log_ay$ 과 같이 곱을 합으로서 다루기 위함이다.





