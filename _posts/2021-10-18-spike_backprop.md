---
title:  "Enabling Spike-Based Backpropagation For Training Deep Neural Network Architectures"

categories:

  - Paper Review
last_modified_at: 2021-10-12T08:06:00-05:00

---



해당 글은
<br/>
Enabling Spike-Based Backpropagation For Training Deep Neural Network Architectures 
<br/>
(https://arxiv.org/abs/1903.06379) 
<br/>
라는 논문을 소개하는 글이다.
<br/>
<br/>
<br/>
<br/>
**ABSTRACT**
<br/>
*"스파이킹 신경망(SNN)는 최근 눈에 띄는 신경 컴퓨팅 패러다임으로 부상하고 있다.
일반적인 얕은 SNN 아키텍처는 복잡한 표현을 표현할 수 있는 양이 제한적이지만, 입력 스파이크를 사용하여 심층 SNN을 학습시키는 것은 지금까지 성공하지 못했다. 
이 문제를 해결하기 위해 기성 훈련된 심층 인공 신경망을 SNN으로 전환하는 것과 같은 다양한 방법이 제안되었다. 
그러나 ANN-SNN 변환 체계는 스파이킹 시스템의 시간의 역학적 특성을 포착하지 못한다. 
한편, 스파이크 생성 함수의 불연속적이고 차별화 불가능한 특성 때문에 입력 스파이크 이벤트를 사용하여 심층 SNN을 직접 학습시키는 것은 여전히 어려운 문제이다. 
이 문제를 극복하기 위해 LIF 뉴런의 Leaky Behavior를 설명하는 대략적인 파생 방법을 제안한다. 
이 방법은 스파이크 기반 역전파를 사용하여(입력 스파이크 이벤트를 통해) 심층 컨볼루션 SNN을 직접 학습시킬 수 있다. 
우리의 실험은 스파이크 기반 학습으로 훈련된 다른 SNN에 비해 MNIST, SVHN 및 CIFAR-10 데이터 세트에서 최고의 분류 정확도를 달성함으로써 심층 네트워크(VGG 및 잔류 아키텍처)에 대한 제안된 스파이크 기반 학습의 효과를 보여준다. 
또한 스파이킹 도메인에서 추론 연산을 위해 제안된 SNN 훈련 방법의 효과를 입증하기 위해 희소 이벤트 기반 계산을 분석한다."*
<br/>
<br/>
스파이킹 신경망이란 인간 뇌의 실제 뉴런 구조를 모방함으로서 생물학적인 신경망을 최대한 모사하는 신경망 모델이다.
인간의 뇌는 각각의 뉴런들이 주고받는 '스파이크'에 의해 신호들을 주고받는다.
'스파이크'란, 이름처럼 급격하게 증가하는 전기적 신호를 일컫는 말이다.
뉴런을 관찰한 결과, 뉴런의 전위가 특정 상태에서 급격하게 치솟는 양상을 띈다. 
뉴런이 또다른 뉴런으로부터 전기적를 받으면, 
그리고 전기신호를 받음으로서 뉴런이 활성화되면,
그 뉴런또한 스파이크를 발생시켜 전기신호를 다른 뉴런으로 보낸다.
이러한 과정들의 연속으로 뇌 속의 신경들의 상호작용이 진행된다.
<br/>
<br/>
뉴런의 전위가 서서히 증가하다가 임계값을 넘으면 스파이크를 발생시키는데,
이러한 뉴런을 IF(Integrate and Fire) 뉴런이라고 한다.
IF 뉴런은 입력이 들어오면 입력에 따라 뉴런의 막전위가 증가한다.
그렇게 계속해서 증가하다가 입계값을 넘음으로서 스파이크를 발생시키고 막전위는 초기화된다.
이렇게 전위가 쌓이고, 스파이크한 후에 전위가 급격히 떨어져 휴식기간을 가진다.
그리고 IF 뉴런에서 Leaky한 효과가 추가된 것을 LIF(Leaky Integrate and Fire) 뉴런이라고 한다.
Leaky란 누수, 누출을 말하는데, 입력이 없을 때 막전위가 점점 감소(누출)한다는 것이다.
<br/>
<br/>
LIF 뉴런이 발화하는 과정을 그림으로 나타내면 아래와 같다.

![](/assets/image/LIF_graph.png)


pre-spikes는 이전 뉴런이 발생시킨 스파이크이며, 가중치 w가 곱해져 입력으로 들어온다.
입력이 들어오면 LIF 뉴런의 전위도 높아진다.
가운데는 입력을 받은 LIF의 막전위 그래프인데, $V_{mem}$는 Membrane potential(막전위)을 나타내는 변수이다.
time은 이름 그대로 시간의 흐름을 나타내며, $V_{th}$는 Threshold(임계치)를 의미한다.
막전위($V_{mem}$)가 임계 전위($V_{th}$)를 넘으면 스파이크를 발생시켜 다음 뉴런으로 보낸다.
post-spikes는 현재의 LIF 뉴런이 일으킨 스파이크로, 다음 뉴런으로 전달된다.

LIF 뉴런을 수식으로 나타내면 아래와 같다.

$\tau_m \frac{dV_{mem}}{dt} = -V_{mem} + I(t)$

좌변은 막전위에 대한 시간의 미분이다. 
즉, 시간의 변화에 따른 막전위의 변화인데, 막전위의 변화 그래프에서의 기울기로 볼 수 있다.

미분한 값이 기울기이므로 막전위가 증가/감소하는 경사도를 나타낸다.
미분한 값이 양의 방향으로 크다면 가파르게 막전위가 증가하는 것이고 
미분한 값이 음의 방향으로 크다면 가파르게 막전위가 감소하는 것이다.

우변은 막전위와 입력의 차이이다.
입력이 현재 막전위보다 더 크다면, 즉 $V_{mem} < I(t)$ 이라면,
$V_{mem}$을 이항하면
$0 < -V_{mem} + I(t)$ 이다.
우변이 양수이므로 좌변의 미분한 값도 양수인데, 
기울기가 양수라는 것은 증가하고 있다는 말이다.
직관적으로 생각해봐도 입력이 현재 전위보다 높으면 입력이 들어왔으니 전위는 증가한다.
따라서 위 식이 나타내는 의미는 "입력과 현재 막전위의 차이만큼 기울기가 책정된다" 가 된다.
막전위와 입력의 차이(부호 상관 없이 절대값을 말한다)가 크다면 기울기의 절대값이 크므로 가파르게 증감하는 것이다.
막전위와 입력의 차이가 크면 빠르며 그 차이를 좁히는 방향으로, 차이가 적으면 천천히 그 차이를 좁히는 방향으로 막전위가 변화한다.
만약 입력이 현재 막전위보다 작아서(혹은 입력이 없어서) $V_{mem} > I(t)$ 이라면  $0 > -V_{mem} + I(t)$으로 음수이다.
우변(차이)이 음수이므로 좌변(기울기)도 음수이며, 입력이 적거나 없을 때 감소한다는 뜻이다. Leaky의 의미가 이 감소를 말한다.

그리고 좌변의 미분값에 곱해진 타우($\tau_m$)는 감가율인데, 이 값이 클수록 값이 감소하는 Leaky한 효과가 커진다.
그리고 LIF 뉴런으로 들어오는 입력 $I(t)$는 아래 식으로 표현된다.

$I(t) = \sum_{i=1}^{n^l} \big( w_i \sum_{K}^{} \theta_i(t - t_k)   \big)$

i는 이전 뉴런의 번호를 나타낸다. 즉 i번째 이전 뉴런(i-th pre-neuron)을 말한다.
$n^l$은 l번째 레이어(층)의 뉴런 개수를 말한다. i번째 뉴런부터 $n^l$번째(마지막) 뉴런까지, 모든 이전 뉴런에 대하여 괄호 안의 값을 다 더하라는 것이다.
괄호 안의 $w_i \sum_{k}^{} \theta_i(t - t_k)$ 에서 세타함수 $\theta_i(t-t_k)$는 i번째 이전 뉴런에서 입력으로 들어오는 스파이크에 관한 함수이다. 
이 스파이크들을 모두 더해서 i번째 가중치 $w_i$와 곱한다.
이것을 모든 뉴런에 대해 다 더한다는 말이다.
세타함수가 스파이크에 관한 함수라고 했는데, 아래처럼 표현된다.

$\theta_i(t-t_k) = \begin{array}
      1 (t = t_k)\\
      0 (t \neq t_k)
    \end{array}$






