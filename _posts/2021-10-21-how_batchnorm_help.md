---
title:  "How Does Batch Normalization Help Optimization"

categories:

  - Paper Review
last_modified_at: 2021-10-12T08:06:00-05:00

---



해당 글은
<br/>
[How Does Batch Normalization Help Optimization](https://arxiv.org/abs/1805.11604) 
<br/>
라는 논문을 소개하는 글이다.
<br/>
<br/>
<br/>
<br/>
**ABSTRACT**
<br/>
*"배치 정규화는 심층 신경망의 빠르고 안정적인 훈련을 가능케 만드는 널리 채택된 기술이다. 이런 확산에도 불구하고, 배치 정규화의 효과에 대한 정확한 이유는 여전히 잘 이해되지 않는다. 이러한 효과는 소위 "내부 공변량 이동"을 줄이기 위해 훈련 중에 층의 입력 분포의 변화를 제어하는 것에서 비롯된다는 것이 일반적인 견해이다. 이 연구에서 우리는 레이어의 입력 분포 안정성이 배치 정규화의 성공과 거의 관련이 없다는 것을 보여준다. 대신, 우리는 배치 정규화가 학습에 미치는 근본적인 영향을 파악한다. 최적화 환경을 훨씬 부드럽게 만든다는 것이다. 이러한 부드러움은 기울기의 보다 예측 가능하고 안정적인 동작을 유도하여 더 빠른 훈련을 가능하게 한다."* 
<br/>
<br/>
<br/>
<br/>

일반적으로 배치 정규화의 효과는 내부 공변량 변화(Internal Covariate Shift, ICS)를 감소시킴으로서 야기된다고 알려져 있다. 하지만 해당 논문에서는 이러한 원인이 효율적인 학습을 만들지 않는다고 주장한다.   

이러한 주장을 위해 해당 논문에서는 배치 정규화와 ICS의 상관관계, 그리고 ICS 감소와 학습 성능 향상의 상관관계, 이렇게 두가지를 파악한다. 

(Q.1) ICS의 감소가 학습 성능 향상에 도움을 주는가? 
<br/>
(Q.2) 배치 정규화가 ICS 감소에 도움을 주는가? 

먼저 Q.1의 의문에 대하여 실험을 진행한다. 

![](/assets/image/bn_ex1.png)

위의 실험 결과에서 볼 수 있듯이 배치 정규화를 사용했을 때 성능이 크게 향상되지만, 배치 정규화를 사용했을 때와 그렇지 않을 때의 분포 차이는 뚜렷하지 않다. 이로서 우리는 ICS가 학습에 악영향을 미친다는 주장의 신빙성을 의심하게 된다. 

그리고 우리는 아래와 같은 실험을 설계한다. 

1. 배치 정규화를 진행한 후 노이즈를 주입한다. 
2. 이러한 노이즈 주입은 분포를 불안정하고 가변적으로 만들어 ICS 증가에 큰 기여를 할 것이다.
3. ICS 증가에 따른 학습 성능의 변화를 관찰한다.

![](/assets/image/bn_ex2.png)

배치 정규화 이후 노이즈가 주입되었더라도 일반적인 배치 정규화 수행 후의 성능과 비교하여 큰 차이를 보이지 않으며, 둘 모두 배치 정규화를 사용하지 않은 경우에 비해 높은 성능을 보여주고 있다. 위 실험으로 우리는 ICS가 학습 성능에 거의 영향을 미치지 않는다는 알 수 있다. 

이제 우리는 Q.2의 의문에 답하기 위하여 실험을 진행한다. 

아래는 코사인 각도와 $l_2-difference$를 기준으로 하여서, 배치 정규화를 사용했을 때와 사용하지 않았을 때의 ICS 차이를 측정한 것이다. 

![](/assets/image/bn_ex3.png)

이상적인 ICS(적은 ICS)에서, 코사인 각도는 이상적으로 1, $l_2-difference$는 이상적으로 0의 값을 가진다. 하지만 위 실험 결과로 배치 정규화를 사용했을 때 비슷하거나, 오히려 더 안좋은 ICS를 가진다는 것을 확인할 수 있다. 이로서 우리는 배치 정규화와 ICS 감소의 상관관계에 의문을 품게 된다. 

아래와 같은 ICS 측정을 제안한다. 

$L$: 손실

$W_1^{(t)}, … , W_k^{(t)}$: k개의 레이더에 대하여 각 레이더의 매개변수

$x^{(t)}, y^{(t)}$: 시간 t에서 네트워크를 훈련시키는 데 사용되는 입력 라벨 쌍의 배치

시간 t에서 activation i의 ICS를 다음과 같이 정의한다. 

$ICS \; = \; {\parallel G_{t, i} \; - \; {G \prime }_{t, i} \parallel}_2$

G는 레이어의 기울기(Gradient)를 말하며, 위 식은 레이어의 학습 전과 후의 기울기 차이를 ICS로 정의하겠다는 것이다. 각 G는 아래처럼 나타낼 수 있다. 

$G_{t,i} = {\nabla}_{W_i^{(t)}} $
<br/>
$$




