---
title:  "신경망의 학습"

categories:
  - AI
last_modified_at: 2021-10-12T08:06:00-05:00

---



우리는 이제 신경망이 어떻게 동작하는지 알고있다.

입력에 대해 가중치를 곱하고 편향을 더하는 연산을 하여 출력을 계산한다.

그런데 어떤 문제를 풀기 위해서 신경망을 만들기만 하면 되는 것이 아니다.

우리가 AND 게이트 문제를 풀었던 것처럼 신경망이 올바르게 작동하는 가중치와 편향을 찾아야 한다.

이 가중치를 찾는 과정을 '학습'이라고 한다.

우리가 익히 알고있는 학습과 같다.

신경망도 학습을 해야 한다.

갓 태어난 아기가 동물들의 이름을 맞출 수는 없는 노릇이다.

ꙹ

예를 들어 어떠한 사진을 입력으로 받아서 그 사진이 강아지인지 고양이인지 맞추는 문제가 있다고 하자.

가중치를 랜덤으로 설정해놓고 신경망을 작동시키면 정답을 맞출 수 있을까?

ꙹ

정답을 맞출 수 있게 올바른 가중치를 찾아야 한다는 것은 알겠다.

그런데 과연 어떻게 할 것인가?

AND 게이트를 맞추는 것처럼 가중치를 사람이 직접 조정할 것인가?

그 질문에 대한 답은 '경사하강법'이다.

ꙹ

우리는 신경망을 가중치와 편향을 매개변수로 하는 일종의 함수로서 생각할 수 있다.

입력을 받아서 출력을 내보내므로 함수의 형태를 갖추고 있기 때문이다.

$y = f(x)$

많이 보던 식이다.

당연히 알겠지만 f를 함수라고 하며,

$f(x) = x^2 + 7x + 2$ 와 같이 함수 f가 어떤 연산을 가지는지에 대해 표현하기도 한다.

신경망도 함수로 표현할 수 있다.

$y = f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.)$

신경망을 표현하는 함수 f는 위 식처럼 입력과 가중치, 편향 등의 매개변수를 받아서 출력을 낸다.

f가 하는 연산은 우리가 앞에서 다뤘던 신경망의 연산이 될 것이다.

$f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.) =$ [x에 대한 신경망의 연산]

이렇게 함수의 형태를 띄고 있으므로 미분도 가능하다

미분이 가능하다면 경사하강법을 이용해 학습도 가능하다.

어떤 가중치 w가 있다고 할 때, 손실함수에 대해 w로 미분해준다면 경사(기울기)를 구할 수 있다.

손실함수에 대해 경사하강을 진행한다는 것은 손실함수가 최소값이 되도록 하겠다는 말이다.

손실이 적어지도록, 정답에 가까운 예측을 하는 방향으로 w를 업데이트 해서 학습을 하겠다는 의미인 것이다.

대부분의 신경망 학습은 손실함수에 대한 미분을 이용해서 진행한다.

손실을 줄이는 것을 학습의 지표로 삼는 것이다.

ꙹ

경사하강법을 크게 2가지로 나눌 수 있는데 아래와 같다.

1. 미분하여 기울기를 구하는 부분
2. 그것을 빼는 부분(업데이트 하는 부분)


신경망 학습 또한 마찬가지인데 아래와 같다.

1. 오차(손실함수)에 대한 미분을 구하는 부분
2. 가중치를 업데이트 하는 부분

ꙹ

1, 2 모두 한가지가 아닌 여러가지의 방법이 있다.

ꙹ

1에서 미분을 하는 방법에는 단순 수치 미분(Simple Numerical Differential), 오차 역전파(Error Backpropagation) 등이 있다.

2에서 업데이트 하는 것을 최적화(Optimization)라고 하는데, 최적화를 진행하는 방법, 혹은 장치를 옵티마이저(Optimizer)라고 한다.

이 옵티마이저에도 SGD(우리가 이미 알고있는 단순히 빼는 방법), Ada Grad, Momentum, Adam 등이 있다.

ꙹ

1, 2의 다양한 방법은 다음에 소개하도록 할 것이다. 









