---
title:  "신경망의 학습"

categories:
  - AI
last_modified_at: 2021-10-12T08:06:00-05:00

---



우리는 이제 신경망이 어떻게 동작하는지 알고있다.

입력에 대해 가중치를 곱하고 편향을 더하는 연산을 하여 출력을 계산한다.

그런데 어떤 문제를 풀기 위해서 신경망을 만들기만 하면 되는 것이 아니다.

우리가 AND 게이트 문제를 풀었던 것처럼 신경망이 올바르게 작동하는 가중치와 편향을 찾아야 한다.

이 가중치를 찾는 과정을 '학습'이라고 한다.

우리가 익히 알고있는 학습과 같다.

신경망도 학습을 해야 한다.

갓 태어난 아기가 동물들의 이름을 맞출 수는 없는 노릇이다.

예를 들어 어떠한 사진을 입력으로 받아서 그 사진이 강아지인지 고양이인지 맞추는 문제가 있다고 하자.

가중치를 랜덤으로 설정해놓고 신경망을 작동시키면 정답을 맞출 수 있을까?

아마 높은 확률로 오답을 낼 것이다.

정답을 맞출 수 있게 올바른 가중치를 찾아야 한다는 것은 알겠다.

그런데 과연 어떻게 할 것인가?

AND 게이트를 맞추는 것처럼 가중치를 사람이 직접 조정할 것인가?

그 질문에 대한 답은 '경사하강법'이다.

우리는 신경망을 가중치와 편향을 매개변수로 하는 일종의 함수로서 생각할 수 있다.

입력을 받아서 출력을 내보내므로 함수의 형태를 갖추고 있기 때문이다.

$y = f(x)$

많이 보던 식이다.

당연히 알겠지만 f를 함수라고 하며,

$f(x) = x^2 + 7x + 2$ 와 같이 함수 f가 어떤 연산을 가지는지에 대해 표현하기도 한다.

신경망도 함수로 표현할 수 있다.

$y = f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.)$

신경망을 표현하는 함수 f는 위 식처럼 입력과 가중치, 편향 등의 매개변수를 받아서 출력을 낸다.

f가 하는 연산은 우리가 앞에서 다뤘던 신경망의 연산이 될 것이다.

$f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.) =$ [x에 대한 신경망의 연산]

이렇게 함수의 형태를 띄고 있으므로 미분도 가능하다

미분이 가능하다면 경사하강법을 이용해 학습도 가능하다.

![](/assets/image/loss_weight.png)

위 그래프에서 가로축은 가중치 w이고 세로축은 손실함수값이다.

가중치가 변함에 따라 손실이 늘어나기도, 줄어들기도 하는 것을 볼 수 있다.

저 손실함수는 가중치 한개에 대하여 변화를 보여주는 것이고, 실제로는 가중치가 더 많다.

왜냐하면 실제 손실함수 값은 $Loss \Bigg( f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.) \Bigg)$ 이기 때문이다. 

매개변수가 w 하나가 아니라 $x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.$ 이렇게 많다.

매개변수가 100개라면 함수값을 포함하여 101차원의 공간에서 그래프를 그려야 하지만, 

지금은 이해를 위해 하나의 가중치에 대해 손실함수의 변화 그래프를 그린 것이다.

$y = f(x)$에서의 경사하강을 설명할 때 y가 최소값이 되도록 x를 업데이트 한다고 했는데,

신경망에 이것을 적용하면 $Error = Loss \Bigg( f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.) \Bigg)$에서 손실이 최소값이 되도록 가중치를 업데이트 하는 것이다.

어쨌든, 이제 신경망을 미분 가능한 함수로 표현했으니 임의로 잡은 초기의 w에서 미분하여 경사하강을 진행할 수 있다.

한가지 유의할 점은 신경망을 표현한 함수는 다변수함수라는 것이다.

$y = f(x)$ 같은 경우에는 일변수함수기 때문에 기울기를 그냥 미분하여 $frac{df(x)}{dx}$로 구했는데,

신경망은 여려개의 변수(여러개의 가중치, 편향)가 있는 함수기 때문에 편미분을 이용한다.

다른 변수를 상수 취급(무시)하고 미분을 하는 것인데,

다른 변수를 무시한다는 말은 다른 변수가 함수값에 끼치는 영향은 무시하고 편미분하는 변수가 끼치는 영향만 보겠다는 말이다.

가중치에 대해 미분할 때는 편향은 상수 취급하고 편향에 대해 미분할 때는 그 반대이다.

이렇게 편미분 한 것을 $\frac{\partial f(x)}{\partial x}$로 표현한다. (함수 f에서 다른 매개변수는 생략하였다.)

가중치에 대해 편미분했다면 $\frac{\partial f(w)}{\partial w}$가 될 것이다. (함수 f에서 다른 매개변수는 생략하였다.)

손실함수에 대해 경사하강을 진행한다는 것은 손실함수가 최소값이 되도록 하겠다는 말이다.

손실이 적어지도록, 정답에 가까운 예측을 하는 방향으로 w를 업데이트 해서 학습을 하겠다는 의미인 것이다.

$w ; \colon= w - \alpha \frac{\partial Loss \Bigg( f(x, w_{11}, w_{12}, w_{21}, ... , b_1, b_2 .
.) \Bigg)}{\partial w}$

위 식처럼 가중치를 경사하강법을 이용해 업데이트 함으로서 학습이 진행된다.

손실을 줄이는 것을 학습의 지표로 삼는 것이다.

그리고 경사하강법을 크게 2가지로 나눌 수 있는데 아래와 같다.

1. 미분하여 기울기를 구하는 부분
2. 그것을 빼는 부분(업데이트 하는 부분)

신경망 학습 또한 마찬가지인데 아래와 같다.

1. 오차(손실함수)에 대한 미분을 구하는 부분
2. 가중치를 업데이트 하는 부분

1, 2 모두 한가지가 아닌 여러가지의 방법이 있다.

1에서 미분을 하는 방법에는 단순 수치 미분(Simple Numerical Differential), 오차 역전파(Error Backpropagation) 등이 있다.

2에서 업데이트 하는 것을 최적화(Optimization)라고 하는데, 최적화를 진행하는 방법, 혹은 장치를 옵티마이저(Optimizer)라고 한다.

이 옵티마이저에도 SGD(우리가 이미 알고있는 단순히 빼는 방법), Ada Grad, Momentum, Adam 등이 있다.

오늘은 미분을 해서 학습을 한다는 것만 말했고 정확히 어떻게 미분이 되는지는 다음에 소개할 것이다.

1, 2의 다양한 방법 또한 다음에 소개하겠다.









