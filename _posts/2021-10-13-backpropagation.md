---
title:  "오차 역전파법(Backpropagation)"

categories:
  - AI
last_modified_at: 2021-10-12T08:06:00-05:00

---



이전 글에서 신경망의 학습은 2가지 파트로 나뉜다고 했다. 

손실함수에 대해 기울기를 구하는 부분과 그렇게 구한 기울기를 가지고 업데이트(최적화, Optimization)하는 부분으로 말이다.

오늘은 기울기를 구하는 방법 중 하나인 오차 역전파에 대해 알아보겠다. 

먼저 오차 역전파의 핵심이라고 할 수 있는 연쇄법칙(Chain Rule)을 알아보자.

연쇄법칙이란 합성함수의 미분에서 보았을 것인데, 

$y = f(g(x)$ 와 같은 합성함수가 있을 때, 그 도함수가 다음과 같이 구해진다는 것이다.

$y = f(u), \; u = g(x)$ 라고 하면

$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$

$\frac{df(g(x))}{dx} = \frac{df(g(x))}{dg(x)} \cdot \frac{dg(x)}{dx}$

$\{ f(g(x)) \}' = f'(g(x)) \cdot f(g'(x)) $

g에 대한 미분과 f에 대한 미분을 합성했다고 볼 수도 있지만 다르게 말하면 f•g의 미분을 f와 g의 미분으로 분해한 것이다.

신경망의 연산은 아주 복잡하고 층이 늘어날수록 계산이 길어지기 때문에, 

손실함수에 대한 미분을 바로 구하는 것이 아니라 각 층에 대한 미분으로 분해한 후 계산하여 다시 합성해준다는 것이 오차 역전파의 핵심이다.

이 과정이 뒤 층에서부터 거꾸로 미분한 결과가 전파되는 양상을 띄기 때문에 오차 역전파라고 한다.

이를 위해서는 각 층들을 모두 미분할 수 있어야 하는데, 우리는 아직 모든 층을 배우지 않았다.

그렇게 때문에 활성화함수라는 것을 짚고 넘어가겠다.

 








