---
title:  "신경망의 학습"

categories:
  - Machine Learning 
last_modified_at: 2021-10-12T08:06:00-05:00

---



우리는 이제 신경망이 어떻게 동작하는지 알고있다.
<br/>
입력에 대해 가중치를 곱하고 편향을 더하는 연산을 하여 출력을 계산한다.
<br/>
<br/>
그런데 어떤 문제를 풀기 위해서 신경망을 만들기만 하면 되는 것이 아니다.
우리가 AND 게이트 문제를 풀었던 것처럼 신경망이 올바르게 작동하는 가중치와 편향을 찾아야 한다.
이 가중치를 찾는 과정을 '학습'이라고 한다.
우리가 익히 알고있는 학습과 같다.
신경망도 학습을 해야 한다.
갓 태어난 아기가 동물들의 이름을 맞출 수는 없는 노릇이다.
<br/>
<br/>
예를 들어 어떠한 사진을 입력으로 받아서 그 사진이 강아지인지 고양이인지 맞추는 문제가 있다고 하자.
가중치를 랜덤으로 설정해놓고 신경망을 작동시키면 정답을 맞출 수 있을까?
아마 높은 확률로 오답을 낼 것이다.
<br/>
<br/>
정답을 맞출 수 있게 올바른 가중치를 찾아야 한다는 것은 알겠다.
그런데 과연 어떻게 할 것인가?
AND 게이트를 맞추는 것처럼 가중치를 사람이 직접 조정할 것인가?
<br/>
<br/>
그 질문에 대한 답은 '경사하강법'이다.
우리는 신경망을 가중치와 편향을 매개변수로 하는 일종의 함수로서 생각할 수 있다.
입력을 받아서 출력을 내보내므로 함수의 형태를 갖추고 있기 때문이다.
<br/>
<br/>

$y = f(x)$

많이 보던 식이다.
당연히 알겠지만 f를 함수라고 하며,

$f(x) = x^2 + 7x + 2$ 와 같이 함수 f가 어떤 연산을 가지는지에 대해 표현하기도 한다.

<br/>
<br/>
신경망도 함수로 표현할 수 있다.

$Y = f(X, W, B)$

신경망을 표현하는 함수 f는 위 식처럼 입력과 가중치, 편향 등의 매개변수를 받아서 출력을 낸다.
벡터 또는 행렬을 정의역으로 하는 함수라는 것 외에는 다른 것들과 똑같은 함수이다.
f가 하는 연산은 우리가 앞에서 다뤘던 신경망의 연산이 될 것이다.

$f(X, W, B) =$ [X에 대한 신경망의 연산] = A \cdot W + B

이렇게 함수의 형태를 띄고 있으므로 미분도 가능하다
미분이 가능하다면 경사하강법을 이용해 학습도 가능하다.

![](/assets/image/loss_weight.png)

위 그래프에서 가로축은 가중치 w(가중치 행렬 W의 원소)이고 세로축은 손실함수값이다.
가중치가 변함에 따라 손실이 늘어나기도, 줄어들기도 하는 것을 볼 수 있다.
<br/>
<br/>
위 그래프의 손실함수값은 가중치 한개에 대하여 변화를 보여주는 것이고, 실제로는 가중치는 행렬이므로 가중치가 더 많다.
또한 가중치 말고도 편향(B), 입력(X)을 매개변수로 하는데, 그런 것들을 전부 무시하여 가중치 하나 이외에 다른 매개변수는 생략하고 그린 것이다.
<br/>
<br/>
실제 손실함수 값은 아래처럼 가중치 말고도 다른 입력을 받아서 결정된다.

$Loss \Big( f(X, W, B) \Big)$ 


<br/>
<br/>
$y = f(x)$에서의 경사하강을 설명할 때 y가 최소값이 되도록 x를 업데이트 한다고 했는데,
<br/>
신경망에 이것을 적용하면 $Error = Loss \Big( f(X, W, B) \Big)$에서 Error(오차, 손실)가 최소값이 되도록 가중치를 업데이트 하는 것이다.
<br/>
<br/>
어쨌든, 이제 신경망을 미분 가능한 함수로 표현했으니 임의로 잡은 초기의 w에서 미분하여 경사하강을 진행할 수 있다.
<br/>
<br/>
한가지 유의할 점은 신경망을 표현한 함수는 다변수함수라는 것이다.
$y = f(x)$ 같은 경우에는 일변수함수기 때문에 기울기를 그냥 미분하여 $\frac{df(x)}{dx}$로 구했는데,
신경망은 여려개의 매개변수(여러개의 가중치, 편향)가 있는 함수기 때문에 편미분을 이용한다.
다른 변수를 상수 취급(무시)하고 미분을 하는 것인데,
다른 변수를 무시한다는 말은 다른 매개변수가 함수값에 끼치는 영향은 무시하고 편미분하는 매개변수가 끼치는 영향만 보겠다는 말이다.
<br/>
<br/>
가중치에 대해 편미분할 때는 편향은 상수 취급하여 무시하고 계산을 한다.
편향에 대해 편미분할 때는 그 반대일 것이다.
<br/>
<br/>
아무튼 이렇게 편미분 한 것을 $\frac{\partial f(X, W, B)}{\partial W}$ 로 표현한다. 
위는 가중치 행렬에 대한 편미분이고, 편향에 대해 편미분하면 아래처럼 될 것이다.

$\frac{\partial f(X, W, B)}{\partial B}$ 

손실함수에 대해 경사하강을 진행한다는 것은 손실함수가 최소값이 되도록 하겠다는 말이다.
손실이 적어지도록, 정답에 가까운 예측을 하는 방향으로 가중치를 업데이트 해서 학습을 하겠다는 의미인 것이다.
<br/>
<br/>

$W \; \colon= W - \alpha \frac{\partial Loss \Big( f(X, W, B) \Big)}{\partial W}$

위 식처럼 가중치를 경사하강법을 이용해 업데이트 함으로서 학습이 진행된다.
한가지 명심할 것은 절대 아래와 같이 계산하지 않는다.

$W \; \colon= W - \alpha \frac{\partial f(X, W, B)}{\partial W}$

전자는 손실함수에 대해 편미분한 것이고, 후자는 신경망함수에 대하여 편미분한 것이다.
그리고 경사하강은 미분하는 함수의 함수값이 작아지도록 업데이트를 하는 것이다.
저 식대로라면 우리는 손실을 줄이는게 아니라 신경망의 출력을 줄이게 된다.
왜냐하면 $f(X, W, B)$는 신경망을 함수로 나타냈을 때의 함수값이고, 다른 말로는 신경망의 출력이기 때문이다.
우리는 손실을 줄이고 싶기 때문에 손실함수에 대해 미분해야한다.
<br/>
<br/>
손실을 줄이는 것을 학습의 지표로 삼는 것이다.
<br/>
<br/>
그리고 경사하강법을 크게 2가지로 나눌 수 있는데 아래와 같다.

<br/>
<br/>

1. 미분하여 기울기를 구하는 부분
2. 그것을 빼는 부분(업데이트 하는 부분)

<br/>
<br/>
<br/>

신경망 학습 또한 마찬가지인데 아래와 같다.

<br/>
<br/>

1. 오차(손실함수)에 대한 미분을 구하는 부분
2. 가중치를 업데이트 하는 부분

<br/>
<br/>

1, 2 모두 한가지가 아닌 여러가지의 방법이 있다.
<br/>
<br/>
1에서 미분을 하는 방법에는 단순 수치 미분(Simple Numerical Differential), 오차 역전파(Error Backpropagation) 등이 있다.
<br/>
<br/>
2에서 업데이트 하는 것을 최적화(Optimization)라고 하는데, 최적화를 진행하는 방법, 혹은 장치를 옵티마이저(Optimizer)라고 한다.
이 옵티마이저에도 SGD(우리가 이미 알고있는 단순히 빼는 방법), Ada Grad, Momentum, Adam 등이 있다.
<br/>
<br/>
오늘은 미분을 해서 학습을 한다는 것만 말했고 정확히 어떻게 미분이 되는지는 다음에 소개할 것이다.
1, 2의 다양한 방법 또한 다음에 소개하겠다.









